{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Neural Network Implementation using Scikit-Learn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "cancer = load_breast_cancer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'target', 'frame', 'target_names', 'DESCR', 'feature_names', 'filename'])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 30)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cancer['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the X and y from the data\n",
    "X = cancer['data']\n",
    "y = cancer['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create train and test split from X, y\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, \n",
    "                                                    stratify=y,\n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler()"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit only to the training data\n",
    "scaler.fit(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply the trasformation to data (train and test)\n",
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(426, 30)\n",
      "(143, 30)\n"
     ]
    }
   ],
   "source": [
    "#print the shape of X_train and X_test\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's use the MLP classifier available within Scikit-Learn library\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this will create a network with Input-Hid.Layer1-Hid.Layer2-Hid.Layer3-y structure\n",
    "#There are 5, 10, 15 hidden units (also called neurons) in these hidden layers respectively.\n",
    "#all other parameters are default values.\n",
    "mlp = MLPClassifier(activation='relu',\n",
    "                    alpha=0.001,\n",
    "                    batch_size='auto', \n",
    "                    beta_1=0.9,\n",
    "                    beta_2=0.999,\n",
    "                    early_stopping=False, \n",
    "                    epsilon=1e-08,\n",
    "                    hidden_layer_sizes=(5, 10, 15), \n",
    "                    learning_rate='constant',\n",
    "                    learning_rate_init=0.001, \n",
    "                    max_iter=1000, \n",
    "                    momentum=0.9,\n",
    "                    nesterovs_momentum=True,\n",
    "                    power_t=0.5, \n",
    "                    random_state=42,\n",
    "                    shuffle=True, \n",
    "                    solver='adam', \n",
    "                    tol=0.0001, \n",
    "                    validation_fraction=0.1,\n",
    "                    verbose=True, \n",
    "                    warm_start=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, loss = 0.71239917\n",
      "Iteration 2, loss = 0.70133007\n",
      "Iteration 3, loss = 0.69243420\n",
      "Iteration 4, loss = 0.68414228\n",
      "Iteration 5, loss = 0.67679876\n",
      "Iteration 6, loss = 0.67064683\n",
      "Iteration 7, loss = 0.66482222\n",
      "Iteration 8, loss = 0.65925248\n",
      "Iteration 9, loss = 0.65333629\n",
      "Iteration 10, loss = 0.64808842\n",
      "Iteration 11, loss = 0.64294777\n",
      "Iteration 12, loss = 0.63735517\n",
      "Iteration 13, loss = 0.63148324\n",
      "Iteration 14, loss = 0.62535144\n",
      "Iteration 15, loss = 0.61886209\n",
      "Iteration 16, loss = 0.61158739\n",
      "Iteration 17, loss = 0.60357024\n",
      "Iteration 18, loss = 0.59492558\n",
      "Iteration 19, loss = 0.58584755\n",
      "Iteration 20, loss = 0.57527345\n",
      "Iteration 21, loss = 0.56436068\n",
      "Iteration 22, loss = 0.55242097\n",
      "Iteration 23, loss = 0.53941054\n",
      "Iteration 24, loss = 0.52603042\n",
      "Iteration 25, loss = 0.51183346\n",
      "Iteration 26, loss = 0.49641982\n",
      "Iteration 27, loss = 0.48121081\n",
      "Iteration 28, loss = 0.46539377\n",
      "Iteration 29, loss = 0.44976364\n",
      "Iteration 30, loss = 0.43377123\n",
      "Iteration 31, loss = 0.41790701\n",
      "Iteration 32, loss = 0.40245164\n",
      "Iteration 33, loss = 0.38710642\n",
      "Iteration 34, loss = 0.37199329\n",
      "Iteration 35, loss = 0.35736919\n",
      "Iteration 36, loss = 0.34355649\n",
      "Iteration 37, loss = 0.33044882\n",
      "Iteration 38, loss = 0.31808505\n",
      "Iteration 39, loss = 0.30621889\n",
      "Iteration 40, loss = 0.29500195\n",
      "Iteration 41, loss = 0.28413194\n",
      "Iteration 42, loss = 0.27396349\n",
      "Iteration 43, loss = 0.26433933\n",
      "Iteration 44, loss = 0.25513679\n",
      "Iteration 45, loss = 0.24630580\n",
      "Iteration 46, loss = 0.23825712\n",
      "Iteration 47, loss = 0.23048802\n",
      "Iteration 48, loss = 0.22336192\n",
      "Iteration 49, loss = 0.21644175\n",
      "Iteration 50, loss = 0.20974473\n",
      "Iteration 51, loss = 0.20359849\n",
      "Iteration 52, loss = 0.19755563\n",
      "Iteration 53, loss = 0.19197425\n",
      "Iteration 54, loss = 0.18672497\n",
      "Iteration 55, loss = 0.18153105\n",
      "Iteration 56, loss = 0.17655156\n",
      "Iteration 57, loss = 0.17180812\n",
      "Iteration 58, loss = 0.16735081\n",
      "Iteration 59, loss = 0.16302793\n",
      "Iteration 60, loss = 0.15902877\n",
      "Iteration 61, loss = 0.15529733\n",
      "Iteration 62, loss = 0.15152358\n",
      "Iteration 63, loss = 0.14780052\n",
      "Iteration 64, loss = 0.14436303\n",
      "Iteration 65, loss = 0.14105844\n",
      "Iteration 66, loss = 0.13781900\n",
      "Iteration 67, loss = 0.13481312\n",
      "Iteration 68, loss = 0.13176729\n",
      "Iteration 69, loss = 0.12897054\n",
      "Iteration 70, loss = 0.12603301\n",
      "Iteration 71, loss = 0.12342844\n",
      "Iteration 72, loss = 0.12087875\n",
      "Iteration 73, loss = 0.11823499\n",
      "Iteration 74, loss = 0.11578339\n",
      "Iteration 75, loss = 0.11341890\n",
      "Iteration 76, loss = 0.11114719\n",
      "Iteration 77, loss = 0.10906091\n",
      "Iteration 78, loss = 0.10687518\n",
      "Iteration 79, loss = 0.10468219\n",
      "Iteration 80, loss = 0.10275134\n",
      "Iteration 81, loss = 0.10058794\n",
      "Iteration 82, loss = 0.09875935\n",
      "Iteration 83, loss = 0.09709459\n",
      "Iteration 84, loss = 0.09527932\n",
      "Iteration 85, loss = 0.09362687\n",
      "Iteration 86, loss = 0.09187513\n",
      "Iteration 87, loss = 0.09026946\n",
      "Iteration 88, loss = 0.08868551\n",
      "Iteration 89, loss = 0.08703175\n",
      "Iteration 90, loss = 0.08556986\n",
      "Iteration 91, loss = 0.08412299\n",
      "Iteration 92, loss = 0.08267159\n",
      "Iteration 93, loss = 0.08124618\n",
      "Iteration 94, loss = 0.07983040\n",
      "Iteration 95, loss = 0.07853627\n",
      "Iteration 96, loss = 0.07730532\n",
      "Iteration 97, loss = 0.07628478\n",
      "Iteration 98, loss = 0.07529141\n",
      "Iteration 99, loss = 0.07420321\n",
      "Iteration 100, loss = 0.07317681\n",
      "Iteration 101, loss = 0.07217283\n",
      "Iteration 102, loss = 0.07130186\n",
      "Iteration 103, loss = 0.07035637\n",
      "Iteration 104, loss = 0.06954061\n",
      "Iteration 105, loss = 0.06873900\n",
      "Iteration 106, loss = 0.06799698\n",
      "Iteration 107, loss = 0.06717041\n",
      "Iteration 108, loss = 0.06654095\n",
      "Iteration 109, loss = 0.06594092\n",
      "Iteration 110, loss = 0.06519675\n",
      "Iteration 111, loss = 0.06445539\n",
      "Iteration 112, loss = 0.06380191\n",
      "Iteration 113, loss = 0.06302663\n",
      "Iteration 114, loss = 0.06234902\n",
      "Iteration 115, loss = 0.06165272\n",
      "Iteration 116, loss = 0.06100765\n",
      "Iteration 117, loss = 0.06050550\n",
      "Iteration 118, loss = 0.05982606\n",
      "Iteration 119, loss = 0.05939048\n",
      "Iteration 120, loss = 0.05894084\n",
      "Iteration 121, loss = 0.05816501\n",
      "Iteration 122, loss = 0.05745072\n",
      "Iteration 123, loss = 0.05679382\n",
      "Iteration 124, loss = 0.05612684\n",
      "Iteration 125, loss = 0.05555440\n",
      "Iteration 126, loss = 0.05507934\n",
      "Iteration 127, loss = 0.05447748\n",
      "Iteration 128, loss = 0.05408458\n",
      "Iteration 129, loss = 0.05360114\n",
      "Iteration 130, loss = 0.05304922\n",
      "Iteration 131, loss = 0.05252272\n",
      "Iteration 132, loss = 0.05208113\n",
      "Iteration 133, loss = 0.05153857\n",
      "Iteration 134, loss = 0.05107431\n",
      "Iteration 135, loss = 0.05070428\n",
      "Iteration 136, loss = 0.05033536\n",
      "Iteration 137, loss = 0.04996209\n",
      "Iteration 138, loss = 0.04960372\n",
      "Iteration 139, loss = 0.04920766\n",
      "Iteration 140, loss = 0.04883700\n",
      "Iteration 141, loss = 0.04847123\n",
      "Iteration 142, loss = 0.04794583\n",
      "Iteration 143, loss = 0.04765533\n",
      "Iteration 144, loss = 0.04727094\n",
      "Iteration 145, loss = 0.04683241\n",
      "Iteration 146, loss = 0.04649712\n",
      "Iteration 147, loss = 0.04614784\n",
      "Iteration 148, loss = 0.04580186\n",
      "Iteration 149, loss = 0.04547903\n",
      "Iteration 150, loss = 0.04518429\n",
      "Iteration 151, loss = 0.04487409\n",
      "Iteration 152, loss = 0.04463593\n",
      "Iteration 153, loss = 0.04448460\n",
      "Iteration 154, loss = 0.04418675\n",
      "Iteration 155, loss = 0.04394370\n",
      "Iteration 156, loss = 0.04362346\n",
      "Iteration 157, loss = 0.04326990\n",
      "Iteration 158, loss = 0.04304445\n",
      "Iteration 159, loss = 0.04268811\n",
      "Iteration 160, loss = 0.04240048\n",
      "Iteration 161, loss = 0.04209902\n",
      "Iteration 162, loss = 0.04193905\n",
      "Iteration 163, loss = 0.04163125\n",
      "Iteration 164, loss = 0.04142888\n",
      "Iteration 165, loss = 0.04115748\n",
      "Iteration 166, loss = 0.04096606\n",
      "Iteration 167, loss = 0.04069486\n",
      "Iteration 168, loss = 0.04030733\n",
      "Iteration 169, loss = 0.03998151\n",
      "Iteration 170, loss = 0.03967512\n",
      "Iteration 171, loss = 0.03936314\n",
      "Iteration 172, loss = 0.03911235\n",
      "Iteration 173, loss = 0.03878604\n",
      "Iteration 174, loss = 0.03856375\n",
      "Iteration 175, loss = 0.03829844\n",
      "Iteration 176, loss = 0.03805236\n",
      "Iteration 177, loss = 0.03782612\n",
      "Iteration 178, loss = 0.03765131\n",
      "Iteration 179, loss = 0.03748288\n",
      "Iteration 180, loss = 0.03729315\n",
      "Iteration 181, loss = 0.03711178\n",
      "Iteration 182, loss = 0.03687144\n",
      "Iteration 183, loss = 0.03662297\n",
      "Iteration 184, loss = 0.03635056\n",
      "Iteration 185, loss = 0.03612602\n",
      "Iteration 186, loss = 0.03596925\n",
      "Iteration 187, loss = 0.03567841\n",
      "Iteration 188, loss = 0.03545677\n",
      "Iteration 189, loss = 0.03526154\n",
      "Iteration 190, loss = 0.03512311\n",
      "Iteration 191, loss = 0.03490636\n",
      "Iteration 192, loss = 0.03471597\n",
      "Iteration 193, loss = 0.03440533\n",
      "Iteration 194, loss = 0.03417045\n",
      "Iteration 195, loss = 0.03388363\n",
      "Iteration 196, loss = 0.03367942\n",
      "Iteration 197, loss = 0.03359774\n",
      "Iteration 198, loss = 0.03355855\n",
      "Iteration 199, loss = 0.03347781\n",
      "Iteration 200, loss = 0.03336791\n",
      "Iteration 201, loss = 0.03307962\n",
      "Iteration 202, loss = 0.03280584\n",
      "Iteration 203, loss = 0.03257961\n",
      "Iteration 204, loss = 0.03231086\n",
      "Iteration 205, loss = 0.03209233\n",
      "Iteration 206, loss = 0.03184047\n",
      "Iteration 207, loss = 0.03158632\n",
      "Iteration 208, loss = 0.03144429\n",
      "Iteration 209, loss = 0.03120465\n",
      "Iteration 210, loss = 0.03097397\n",
      "Iteration 211, loss = 0.03080523\n",
      "Iteration 212, loss = 0.03072445\n",
      "Iteration 213, loss = 0.03038955\n",
      "Iteration 214, loss = 0.03032104\n",
      "Iteration 215, loss = 0.03014196\n",
      "Iteration 216, loss = 0.02997365\n",
      "Iteration 217, loss = 0.02996482\n",
      "Iteration 218, loss = 0.02989336\n",
      "Iteration 219, loss = 0.02978380\n",
      "Iteration 220, loss = 0.02960171\n",
      "Iteration 221, loss = 0.02917632\n",
      "Iteration 222, loss = 0.02879053\n",
      "Iteration 223, loss = 0.02840381\n",
      "Iteration 224, loss = 0.02794275\n",
      "Iteration 225, loss = 0.02837075\n",
      "Iteration 226, loss = 0.02797349\n",
      "Iteration 227, loss = 0.02779302\n",
      "Iteration 228, loss = 0.02764784\n",
      "Iteration 229, loss = 0.02747259\n",
      "Iteration 230, loss = 0.02730398\n",
      "Iteration 231, loss = 0.02722984\n",
      "Iteration 232, loss = 0.02701954\n",
      "Iteration 233, loss = 0.02661791\n",
      "Iteration 234, loss = 0.02635171\n",
      "Iteration 235, loss = 0.02612173\n",
      "Iteration 236, loss = 0.02587836\n",
      "Iteration 237, loss = 0.02580877\n",
      "Iteration 238, loss = 0.02558836\n",
      "Iteration 239, loss = 0.02542002\n",
      "Iteration 240, loss = 0.02527152\n",
      "Iteration 241, loss = 0.02509293\n",
      "Iteration 242, loss = 0.02497473\n",
      "Iteration 243, loss = 0.02480860\n",
      "Iteration 244, loss = 0.02462400\n",
      "Iteration 245, loss = 0.02444715\n",
      "Iteration 246, loss = 0.02428740\n",
      "Iteration 247, loss = 0.02416662\n",
      "Iteration 248, loss = 0.02400654\n",
      "Iteration 249, loss = 0.02385616\n",
      "Iteration 250, loss = 0.02372875\n",
      "Iteration 251, loss = 0.02359015\n",
      "Iteration 252, loss = 0.02338685\n",
      "Iteration 253, loss = 0.02330844\n",
      "Iteration 254, loss = 0.02317788\n",
      "Iteration 255, loss = 0.02314942\n",
      "Iteration 256, loss = 0.02303499\n",
      "Iteration 257, loss = 0.02295603\n",
      "Iteration 258, loss = 0.02283496\n",
      "Iteration 259, loss = 0.02271923\n",
      "Iteration 260, loss = 0.02256681\n",
      "Iteration 261, loss = 0.02238252\n",
      "Iteration 262, loss = 0.02224973\n",
      "Iteration 263, loss = 0.02213207\n",
      "Iteration 264, loss = 0.02201745\n",
      "Iteration 265, loss = 0.02187577\n",
      "Iteration 266, loss = 0.02172741\n",
      "Iteration 267, loss = 0.02159722\n",
      "Iteration 268, loss = 0.02137408\n",
      "Iteration 269, loss = 0.02119827\n",
      "Iteration 270, loss = 0.02125251\n",
      "Iteration 271, loss = 0.02106461\n",
      "Iteration 272, loss = 0.02092720\n",
      "Iteration 273, loss = 0.02065676\n",
      "Iteration 274, loss = 0.02091856\n",
      "Iteration 275, loss = 0.02068900\n",
      "Iteration 276, loss = 0.02045947\n",
      "Iteration 277, loss = 0.02023436\n",
      "Iteration 278, loss = 0.01996988\n",
      "Iteration 279, loss = 0.01978606\n",
      "Iteration 280, loss = 0.01978688\n",
      "Iteration 281, loss = 0.01963765\n",
      "Iteration 282, loss = 0.01935619\n",
      "Iteration 283, loss = 0.01926903\n",
      "Iteration 284, loss = 0.01920952\n",
      "Iteration 285, loss = 0.01910323\n",
      "Iteration 286, loss = 0.01904448\n",
      "Iteration 287, loss = 0.01893725\n",
      "Iteration 288, loss = 0.01882041\n",
      "Iteration 289, loss = 0.01872230\n",
      "Iteration 290, loss = 0.01860738\n",
      "Iteration 291, loss = 0.01866984\n",
      "Iteration 292, loss = 0.01867843\n",
      "Iteration 293, loss = 0.01858241\n",
      "Iteration 294, loss = 0.01835602\n",
      "Iteration 295, loss = 0.01811317\n",
      "Iteration 296, loss = 0.01781863\n",
      "Iteration 297, loss = 0.01755916\n",
      "Iteration 298, loss = 0.01748212\n",
      "Iteration 299, loss = 0.01752588\n",
      "Iteration 300, loss = 0.01746200\n",
      "Iteration 301, loss = 0.01744906\n",
      "Iteration 302, loss = 0.01739334\n",
      "Iteration 303, loss = 0.01725510\n",
      "Iteration 304, loss = 0.01712705\n",
      "Iteration 305, loss = 0.01698432\n",
      "Iteration 306, loss = 0.01695987\n",
      "Iteration 307, loss = 0.01692227\n",
      "Iteration 308, loss = 0.01684755\n",
      "Iteration 309, loss = 0.01677173\n",
      "Iteration 310, loss = 0.01667266\n",
      "Iteration 311, loss = 0.01654970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 312, loss = 0.01636130\n",
      "Iteration 313, loss = 0.01627047\n",
      "Iteration 314, loss = 0.01615395\n",
      "Iteration 315, loss = 0.01610440\n",
      "Iteration 316, loss = 0.01594046\n",
      "Iteration 317, loss = 0.01592651\n",
      "Iteration 318, loss = 0.01580862\n",
      "Iteration 319, loss = 0.01571872\n",
      "Iteration 320, loss = 0.01565289\n",
      "Iteration 321, loss = 0.01557987\n",
      "Iteration 322, loss = 0.01553327\n",
      "Iteration 323, loss = 0.01540769\n",
      "Iteration 324, loss = 0.01534586\n",
      "Iteration 325, loss = 0.01530784\n",
      "Iteration 326, loss = 0.01512521\n",
      "Iteration 327, loss = 0.01511397\n",
      "Iteration 328, loss = 0.01500548\n",
      "Iteration 329, loss = 0.01509014\n",
      "Iteration 330, loss = 0.01498675\n",
      "Iteration 331, loss = 0.01483140\n",
      "Iteration 332, loss = 0.01468887\n",
      "Iteration 333, loss = 0.01478076\n",
      "Iteration 334, loss = 0.01507692\n",
      "Iteration 335, loss = 0.01523326\n",
      "Iteration 336, loss = 0.01505402\n",
      "Iteration 337, loss = 0.01461993\n",
      "Iteration 338, loss = 0.01445641\n",
      "Iteration 339, loss = 0.01429654\n",
      "Iteration 340, loss = 0.01420507\n",
      "Iteration 341, loss = 0.01410610\n",
      "Iteration 342, loss = 0.01402752\n",
      "Iteration 343, loss = 0.01394056\n",
      "Iteration 344, loss = 0.01389240\n",
      "Iteration 345, loss = 0.01384835\n",
      "Iteration 346, loss = 0.01380562\n",
      "Iteration 347, loss = 0.01372392\n",
      "Iteration 348, loss = 0.01369973\n",
      "Iteration 349, loss = 0.01363751\n",
      "Iteration 350, loss = 0.01356303\n",
      "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLPClassifier(alpha=0.001, hidden_layer_sizes=(5, 10, 15), max_iter=1000,\n",
       "              random_state=42, verbose=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#fit the training data\n",
    "mlp.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now that the model is trained, let's try to test it\n",
    "predictions = mlp.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[51  2]\n",
      " [ 3 87]]\n"
     ]
    }
   ],
   "source": [
    "#print the test results\n",
    "from sklearn.metrics import classification_report,confusion_matrix,plot_confusion_matrix\n",
    "print(confusion_matrix(y_test,predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.94      0.96      0.95        53\n",
      "           1       0.98      0.97      0.97        90\n",
      "\n",
      "    accuracy                           0.97       143\n",
      "   macro avg       0.96      0.96      0.96       143\n",
      "weighted avg       0.97      0.97      0.97       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test,predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Homework**\n",
    "\n",
    "Try experimenting with various values of paramters such as: Number of layers, hidden layer sizes, learning rates etc.\n",
    "\n",
    "Note down the performance against each of these changes in parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
